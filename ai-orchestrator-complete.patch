diff --git a/.env.example b/.env.example
new file mode 100644
--- /dev/null
+++ b/.env.example
@@ -0,0 +1,9 @@
+GPU_DEVICE=0
+PORT_QWEN=8000
+PORT_PIPER=5002
+PORT_OMNI=11437
+MODEL_QWEN3=./models/qwen3-30b-instruct-awq
+MODEL_OMNI=./models/Qwen3-Omni-14B
+MODEL_WHISPER=./models/whisper-large-v3
+MODEL_PIPER=./tts/piper/ru_RU-irina-medium.onnx
+INPUT_DIR=./inputs
diff --git a/compose/docker-compose.full.yml b/compose/docker-compose.full.yml
new file mode 100644
--- /dev/null
+++ b/compose/docker-compose.full.yml
@@ -0,0 +1,79 @@
+version: "3.9"
+services:
+  whisper:
+    image: ghcr.io/guillaumekln/faster-whisper:latest
+    container_name: whisper-stt
+    restart: unless-stopped
+    runtime: nvidia
+    environment:
+      - NVIDIA_VISIBLE_DEVICES=all
+      - CUDA_VISIBLE_DEVICES=${GPU_DEVICE:-0}
+    networks: [ai-stack]
+    volumes:
+      - ${MODEL_WHISPER}:/model
+    command: --model /model --device cuda --compute_type float16
+
+  qwen3:
+    image: vllm/vllm-openai:latest
+    container_name: qwen3-llm
+    restart: unless-stopped
+    runtime: nvidia
+    networks: [ai-stack]
+    ports:
+      - "${PORT_QWEN}:8000"
+    environment:
+      - NVIDIA_VISIBLE_DEVICES=all
+      - CUDA_VISIBLE_DEVICES=${GPU_DEVICE:-0}
+    volumes:
+      - ${MODEL_QWEN3}:/models
+    command: >
+      --model /models
+      --dtype auto
+      --quantization awq
+      --max-model-len 8192
+      --gpu-memory-utilization 0.88
+      --tensor-parallel-size 1
+      --port 8000
+    healthcheck:
+      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
+      interval: 30s
+      timeout: 10s
+      retries: 3
+
+  piper:
+    image: rhasspy/piper:latest
+    container_name: piper-tts
+    restart: unless-stopped
+    networks: [ai-stack]
+    ports:
+      - "${PORT_PIPER}:5002"
+    volumes:
+      - ./tts/piper:/voices
+    command: --voice /voices/ru_RU-irina-medium.onnx --port 5002
+
+  qwen3-omni:
+    build:
+      context: ./omni
+      dockerfile: Dockerfile.qwen3-omni
+    container_name: qwen3-omni
+    restart: unless-stopped
+    runtime: nvidia
+    networks: [ai-stack]
+    ports:
+      - "${PORT_OMNI}:11437"
+    volumes:
+      - ${MODEL_OMNI}:/app/model
+      - ./inputs:/app/inputs
+    environment:
+      - NVIDIA_VISIBLE_DEVICES=all
+      - CUDA_VISIBLE_DEVICES=${GPU_DEVICE:-0}
+    healthcheck:
+      test: ["CMD", "curl", "-f", "http://localhost:11437/status"]
+      interval: 30s
+      timeout: 10s
+      retries: 3
+
+networks:
+  ai-stack:
+    driver: bridge
+    internal: true
diff --git a/make/Makefile b/make/Makefile
new file mode 100644
--- /dev/null
+++ b/make/Makefile
@@ -0,0 +1,53 @@
+.PHONY: build run stop restart clean status vram test-image test-audio download-models healthcheck logs
+
+DOCKER_COMPOSE = docker compose -p smart-home-ai
+INPUT_DIR = ./inputs
+PORT_OMNI = ${PORT_OMNI}
+
+build:
+	$(DOCKER_COMPOSE) -f compose/docker-compose.full.yml build
+
+run:
+	$(DOCKER_COMPOSE) -f compose/docker-compose.full.yml up -d
+
+stop:
+	-$(DOCKER_COMPOSE) -f compose/docker-compose.full.yml stop || true
+
+restart:
+	$(DOCKER_COMPOSE) -f compose/docker-compose.full.yml restart
+
+clean:
+	$(DOCKER_COMPOSE) -f compose/docker-compose.full.yml down
+
+status:
+	curl -s http://localhost:$(PORT_OMNI)/status | jq
+
+vram:
+	nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader,nounits | \
+	awk -F, '{printf "VRAM: %d / %d MB (%.1f%%)\n", $$1, $$2, $$1*100/$$2}'
+
+test-image:
+	@test -f "$(INPUT_DIR)/snapshot.jpg" || (echo "‚ùå snapshot.jpg –Ω–µ –Ω–∞–π–¥–µ–Ω" && exit 1)
+	curl -X POST http://localhost:$(PORT_OMNI)/generate \
+	  -H "Content-Type: application/json" \
+	  -d '{"text":"–ß—Ç–æ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏?","image":"'"$$(base64 -w 0 $(INPUT_DIR)/snapshot.jpg)"'"}' | jq
+
+test-audio:
+	@test -f "$(INPUT_DIR)/voice.wav" || (echo "‚ùå voice.wav –Ω–µ –Ω–∞–π–¥–µ–Ω" && exit 1)
+	curl -X POST http://localhost:$(PORT_OMNI)/generate \
+	  -H "Content-Type: application/json" \
+	  -d '{"text":"–ß—Ç–æ —Å–∫–∞–∑–∞–ª —á–µ–ª–æ–≤–µ–∫?","audio":"'"$$(base64 -w 0 $(INPUT_DIR)/voice.wav)"'"}' | jq
+
+download-models:
+	@command -v git-lfs >/dev/null || (echo "‚ùå git-lfs –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω" && exit 1)
+	git lfs install
+	git clone https://huggingface.co/Qwen/Qwen3-30B-Instruct-AWQ ./models/qwen3-30b-instruct-awq || true
+	git clone https://huggingface.co/Qwen/Qwen3-Omni-14B ./models/Qwen3-Omni-14B || true
+	wget https://huggingface.co/rhasspy/piper-voices/resolve/main/ru/ru_RU-irina-medium.onnx -O ./tts/piper/ru_RU-irina-medium.onnx || true
+
+healthcheck:
+	@echo "üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤:"
+	docker inspect --format='.Name: .State.Health.Status' $$(docker ps -q)
+
+logs:
+	$(DOCKER_COMPOSE) -f compose/docker-compose.full.yml logs -f
diff --git a/omni/Dockerfile.qwen3-omni b/omni/Dockerfile.qwen3-omni
new file mode 100644
--- /dev/null
+++ b/omni/Dockerfile.qwen3-omni
@@ -0,0 +1,7 @@
+FROM nvidia/cuda:12.4.0-devel-ubuntu22.04
+WORKDIR /app
+RUN apt update && apt install -y ffmpeg git curl python3-pip libsndfile1
+RUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
+RUN pip install transformers accelerate optimum scipy soundfile flask pillow
+COPY inference_server.py /app/
+CMD ["python3", "inference_server.py"]
diff --git a/omni/inference_server.py b/omni/inference_server.py
new file mode 100644
--- /dev/null
+++ b/omni/inference_server.py
@@ -0,0 +1,34 @@
+from flask import Flask, request, jsonify
+from transformers import AutoProcessor, AutoModelForCausalLM
+import torch, base64, io, soundfile as sf
+from PIL import Image
+
+app = Flask(__name__)
+model_path = "/app/model"
+processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
+model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto", trust_remote_code=True, torch_dtype=torch.bfloat16)
+model.eval()
+
+@app.route("/generate", methods=["POST"])
+def generate():
+    data = request.json
+    text = data.get("text", "")
+    image, audio, sampling_rate = None, None, None
+    if data.get("image"):
+        image = Image.open(io.BytesIO(base64.b64decode(data["image"]))).convert("RGB")
+    if data.get("audio"):
+        audio_array, sampling_rate = sf.read(io.BytesIO(base64.b64decode(data["audio"])))
+        if len(audio_array.shape) > 1: audio_array = audio_array.mean(axis=1)
+        audio = audio_array
+    inputs = processor(text=text, images=image, audios=audio, sampling_rate=sampling_rate, return_tensors="pt").to(model.device)
+    with torch.no_grad():
+        output_ids = model.generate(**inputs, max_new_tokens=256)
+    response = processor.batch_decode(output_ids, skip_special_tokens=True)[0]
+    return jsonify({"response": response})
+
+@app.route("/status", methods=["GET"])
+def status():
+    return jsonify({"status": "ok"})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=11437)
diff --git a/node-red/qwen3-voice-assistant.json b/node-red/qwen3-voice-assistant.json
new file mode 100644
--- /dev/null
+++ b/node-red/qwen3-voice-assistant.json
@@ -0,0 +1,100 @@
+[
+  {
+    "id": "whisper-in",
+    "type": "http in",
+    "name": "Voice Input (WAV)",
+    "url": "/voice-command",
+    "method": "post",
+    "upload": true,
+    "wires": [
+      [
+        "whisper-request"
+      ]
+    ]
+  },
+  {
+    "id": "whisper-request",
+    "type": "http request",
+    "name": "‚Üí Whisper STT",
+    "method": "POST",
+    "url": "http://whisper:9000/asr",
+    "headers": [
+      {
+        "key": "Content-Type",
+        "value": "audio/wav"
+      }
+    ],
+    "wires": [
+      [
+        "qwen-prompt"
+      ]
+    ]
+  },
+  {
+    "id": "qwen-prompt",
+    "type": "function",
+    "name": "Build Qwen Prompt",
+    "func": "msg.payload = { model: 'qwen3-30b-instruct', messages: [{role:'system',content:'–¢—ã ‚Äî –≥–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫ —É–º–Ω–æ–≥–æ –¥–æ–º–∞.'},{role:'user',content:msg.payload}]}; return msg;",
+    "wires": [
+      [
+        "qwen-request"
+      ]
+    ]
+  },
+  {
+    "id": "qwen-request",
+    "type": "http request",
+    "name": "‚Üí Qwen3 LLM",
+    "method": "POST",
+    "url": "http://qwen3:8000/v1/chat/completions",
+    "headers": [
+      {
+        "key": "Content-Type",
+        "value": "application/json"
+      }
+    ],
+    "wires": [
+      [
+        "parse-response"
+      ]
+    ]
+  },
+  {
+    "id": "parse-response",
+    "type": "function",
+    "name": "Parse & Route",
+    "func": "msg.tts_text = msg.payload.choices[0].message.content; return msg;",
+    "wires": [
+      [
+        "piper-request"
+      ]
+    ]
+  },
+  {
+    "id": "piper-request",
+    "type": "http request",
+    "name": "‚Üí Piper TTS",
+    "method": "POST",
+    "url": "http://piper:5002/",
+    "headers": [
+      {
+        "key": "Content-Type",
+        "value": "text/plain"
+      }
+    ],
+    "wires": [
+      [
+        "http-out"
+      ]
+    ]
+  },
+  {
+    "id": "http-out",
+    "type": "http response",
+    "name": "Audio Response (WAV)",
+    "statusCode": "200",
+    "headers": {
+      "content-type": "audio/wav"
+    }
+  }
+]
diff --git a/node-red/omni-trigger.json b/node-red/omni-trigger.json
new file mode 100644
--- /dev/null
+++ b/node-red/omni-trigger.json
@@ -0,0 +1,44 @@
+[
+  {
+    "id": "omni-trigger",
+    "type": "http in",
+    "name": "Omni Trigger",
+    "url": "/omni-analyze",
+    "method": "post",
+    "upload": true,
+    "wires": [
+      [
+        "omni-request"
+      ]
+    ]
+  },
+  {
+    "id": "omni-request",
+    "type": "http request",
+    "name": "‚Üí Qwen3-Omni",
+    "method": "POST",
+    "url": "http://qwen3-omni:11437/generate",
+    "headers": [
+      {
+        "key": "Content-Type",
+        "value": "application/json"
+      }
+    ],
+    "wires": [
+      [
+        "omni-parse"
+      ]
+    ]
+  },
+  {
+    "id": "omni-parse",
+    "type": "function",
+    "name": "Parse Omni Response",
+    "func": "msg.tts_text = msg.payload.response; return msg;",
+    "wires": [
+      [
+        "piper-request"
+      ]
+    ]
+  }
+]
diff --git a/docs/voice-assistant.md b/docs/voice-assistant.md
new file mode 100644
--- /dev/null
+++ b/docs/voice-assistant.md
@@ -0,0 +1,16 @@
+# üéôÔ∏è –ì–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç
+
+## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
+- Whisper ‚Üí Qwen3 ‚Üí Piper
+- JSON-–∫–æ–º–∞–Ω–¥—ã ‚Üí Home Assistant
+
+## –ü–æ—Ç–æ–∫
+1. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≥–æ–≤–æ—Ä–∏—Ç ‚Üí Whisper (STT)
+2. –¢–µ–∫—Å—Ç ‚Üí Qwen3 (LLM)
+3. –û—Ç–≤–µ—Ç ‚Üí Piper (TTS)
+4. –û–∑–≤—É—á–∫–∞ –∏–ª–∏ JSON ‚Üí HA
+
+## –ü—Ä–∏–º–µ—Ä
+```bash
+curl -X POST http://localhost:1880/voice-command --data-binary @voice.wav --output response.wav
+```
diff --git a/docs/omni-module.md b/docs/omni-module.md
new file mode 100644
--- /dev/null
+++ b/docs/omni-module.md
@@ -0,0 +1,71 @@
+# üß† Omni-–º–æ–¥—É–ª—å (Qwen3-Omni)
+
+–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –Ω–∞ –±–∞–∑–µ **Qwen3-Omni**, –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤ —Å—Ç–µ–∫ —É–º–Ω–æ–≥–æ –¥–æ–º–∞. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∞—É–¥–∏–æ. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–æ–¥ –ª–æ–∫–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–∞ GPU —Å —É—á—ë—Ç–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–æ VRAM.
+
+---
+
+## üöÄ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
+- üì∑ –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, snapshot –æ—Ç Frigate)
+- üîä –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∞—É–¥–∏–æ (–≥–æ–ª–æ—Å–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã, —à—É–º–æ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è)
+- üí¨ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –∏–ª–∏ –≥–æ–ª–æ—Å–æ–≤–æ–º –≤–∏–¥–µ
+- üß© –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Node-RED –∏ Home Assistant
+
+---
+
+## ‚öôÔ∏è –ê–ø–ø–∞—Ä–∞—Ç–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
+
+- **GPU**: RTX 4090 (24 –ì–ë VRAM) ‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –¥–ª—è Qwen3-Omni‚Äë14B  
+- **CPU**: Ryzen 7 7800X3D ‚Äî –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±—ã—Å—Ç—Ä—ã–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –∞—É–¥–∏–æ/–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π  
+- **RAM**: 64 –ì–ë ‚Äî –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö  
+
+### üîß –ü—Ä–∏—ë–º—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞–≥—Ä—É–∑–∫–∏
+- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ **AWQ‚Äë–∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏** –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è VRAM‚Äë–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è  
+- –ó–∞–ø—É—Å–∫ Omni **–ø–æ —Å–æ–±—ã—Ç–∏—é** (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–æ–ª—å–∫–æ –ø—Ä–∏ —Ç—Ä–µ–≤–æ–≥–µ –æ—Ç Frigate), –∞ –Ω–µ –≤ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ  
+- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ `max_new_tokens` –∏ `gpu-memory-utilization` –≤ `docker-compose.full.yml`  
+- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ VRAM —á–µ—Ä–µ–∑ `make vram` –∏ –∞–ª–µ—Ä—Ç—ã Grafana/DCGM  
+- Fallback‚Äë–º–µ—Ö–∞–Ω–∏–∑–º: –µ—Å–ª–∏ Omni –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω, –∑–∞–ø—Ä–æ—Å —É—Ö–æ–¥–∏—Ç –≤ Qwen3‚ÄëInstruct (—Ç–µ–∫—Å—Ç‚Äë—Ç–æ–ª—å–∫–æ)
+
+---
+
+## üì¶ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
+
+```mermaid
+graph TD
+  Frigate -->|snapshot.jpg| Omni
+  Microphone -->|voice.wav| Omni
+  Omni -->|response| NodeRED
+  NodeRED -->|TTS| Piper
+  NodeRED -->|JSON| HomeAssistant
+```
+
+---
+
+## üì§ –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤
+
+### üñºÔ∏è –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
+```bash
+curl -X POST http://localhost:11437/generate \
+  -H "Content-Type: application/json" \
+  -d '{"text":"–ß—Ç–æ –Ω–∞ —Ñ–æ—Ç–æ?","image":"'"$(base64 -w 0 snapshot.jpg)"'"}'
+```
+
+### üîâ –ê–Ω–∞–ª–∏–∑ –∞—É–¥–∏–æ
+```bash
+curl -X POST http://localhost:11437/generate \
+  -H "Content-Type: application/json" \
+  -d '{"text":"–ß—Ç–æ —Å–∫–∞–∑–∞–ª —á–µ–ª–æ–≤–µ–∫?","audio":"'"$(base64 -w 0 voice.wav)"'"}'
+```
+
+---
+
+## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
+- `make test-image` ‚Äî –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è  
+- `make test-audio` ‚Äî –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ  
+- `make vram` ‚Äî –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ VRAM  
+- `make healthcheck` ‚Äî —Å—Ç–∞—Ç—É—Å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤  
+
+---
+
+## üìå –ü—Ä–∏–º–µ—á–∞–Ω–∏—è
+- Omni‚Äë–º–æ–¥—É–ª—å –Ω–µ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ ‚Äî –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–æ —Å–æ–±—ã—Ç–∏—é  
+- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –≤–≤–æ–¥: —Ç–µ–∫—Å—Ç + –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ + –∞—É–¥–∏–æ  
+- –ü—Ä–∏ –Ω–µ—Ö–≤–∞—Ç–∫–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–π Qwen3‚ÄëInstruct
diff --git a/docs/makefile-guide.md b/docs/makefile-guide.md
new file mode 100644
--- /dev/null
+++ b/docs/makefile-guide.md
@@ -0,0 +1,10 @@
+# üõ†Ô∏è Makefile Guide
+
+## –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
+- `make build` ‚Äî —Å–æ–±—Ä–∞—Ç—å –æ–±—Ä–∞–∑—ã
+- `make run` ‚Äî –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å—Ç–µ–∫
+- `make stop` ‚Äî –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
+- `make test-image` ‚Äî –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å Omni —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
+- `make test-audio` ‚Äî –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å Omni —Å –∞—É–¥–∏–æ
+- `make vram` ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å VRAM
+- `make healthcheck` ‚Äî —Å—Ç–∞—Ç—É—Å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
diff --git a/README.md b/README.md
index 0000000..1111111 100644
--- a/README.md
+++ b/README.md
@@ -81,6 +81,15 @@ Samba ‚Üí —Å–µ—Ç–µ–≤–æ–π –¥–æ—Å—Ç—É–ø –∫ —Ñ–∞–π–ª–∞–º
 - Samba ‚Üí —Å–µ—Ç–µ–≤–æ–π –¥–æ—Å—Ç—É–ø –∫ —Ñ–∞–π–ª–∞–º
 
 ### üß† Omni-–º–æ–¥—É–ª—å
+
+–í —Å—Ç–µ–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω **Qwen3-Omni** ‚Äî –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç, —Å–ø–æ—Å–æ–±–Ω—ã–π –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å:
+- —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã,
+- –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, snapshot –æ—Ç Frigate),
+- –∞—É–¥–∏–æ (–≥–æ–ª–æ—Å–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã).
+
+Omni –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–æ —Å–æ–±—ã—Ç–∏—é –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–æ–¥ RTX 4090 (24 –ì–ë VRAM).
+
 ---
 
 ## üì¶ –•—Ä–∞–Ω–∏–ª–∏—â–µ
@@ -134,6 +143,8 @@ Maximum autonomy, local AI, visual control, data protection, flexibility
 ## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
 
 - [üìù Limitations & Caveats](docs/limitations.md)
 - [üîÆ Roadmap & Future Plans](docs/roadmap.md)
 - [üìú Rule Engine Specification](docs/automation-spec.md)
 - [üóÉÔ∏è Automation DB Schema](docs/automation-db-schema.md)
 - [üß™ Load Testing Plan](docs/automation-loadtest.md)
 - [üß† Omni Module Documentation](docs/omni-module.md)
+- [üéôÔ∏è Voice Assistant](docs/voice-assistant.md)
+- [üõ†Ô∏è Makefile Guide](docs/makefile-guide.md)
 
 ![Yuri's AI-Powered Docker Server Architecture](https://github.com/TrueTaragane/smart-home-architecture/blob/main/githubusercontent.png?raw=true)
diff --git a/README.en.md b/README.en.md
index 0000000..1111111 100644
--- a/README.en.md
+++ b/README.en.md
@@ -78,6 +78,15 @@ Samba ‚Üí network file access
 - Samba ‚Üí network file access
 
 ### üß† Omni Module
+
+The stack integrates **Qwen3-Omni** ‚Äî a multimodal agent capable of processing:
+- text queries,
+- images (e.g., snapshots from Frigate),
+- audio (voice commands).
+
+Omni is triggered by events and optimized for RTX 4090 (24 GB VRAM).
+
 --- 
 
 ## üì¶ Storage Layout
@@ -131,6 +140,8 @@ Maximum autonomy, local AI, visual control, data protection, flexibility
 ## üìö Additional Materials
 
 - [üìù Limitations & Caveats](docs/limitations.md)
 - [üîÆ Roadmap & Future Plans](docs/roadmap.md)
 - [üß† Omni Module Documentation](docs/omni-module.en.md)
+- [üéôÔ∏è Voice Assistant](docs/voice-assistant.en.md)
+- [üõ†Ô∏è Makefile Guide](docs/makefile-guide.en.md)
 
 ![Yuri's AI-Powered Docker Server Architecture](https://github.com/TrueTaragane/smart-home-architecture/blob/main/githubusercontent.png?raw=true)