# Omni Module Documentation

## ðŸ§  Overview

The Omni Module is the multimodal AI agent at the heart of the Vita system, powered by Qwen3-Omni. It processes text, images, and audio inputs to provide intelligent responses and automate home operations.

## ðŸ—ï¸ Architecture

ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° Omni Ð¼Ð¾Ð´ÑƒÐ»Ñ ÑÐ¾ÑÑ‚Ð¾Ð¸Ñ‚ Ð¸Ð· ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ñ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²:

1. **Input Sources** - Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… (Ñ‚ÐµÐºÑÑ‚, Ð°ÑƒÐ´Ð¸Ð¾, Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ)
2. **Preprocessing** - Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…
3. **Format Conversion** - Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¾Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ…
4. **Model Inference** - Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Qwen3-Omni
5. **Post-processing** - Ð¿Ð¾ÑÑ‚Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²
6. **Response Generation** - Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð°
7. **Action Execution** - Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹

Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ñ…Ð¾Ð´ÑÑ‚ Ñ‡ÐµÑ€ÐµÐ· Ð²ÑÐµ ÑÑ‚Ð°Ð¿Ñ‹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸, Ð½Ð°Ñ‡Ð¸Ð½Ð°Ñ Ñ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð² Ð¸ Ð·Ð°ÐºÐ°Ð½Ñ‡Ð¸Ð²Ð°Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸ÐµÐ¼ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹ Ð² ÑÐ¸ÑÑ‚ÐµÐ¼Ðµ ÑƒÐ¼Ð½Ð¾Ð³Ð¾ Ð´Ð¾Ð¼Ð°.

## ðŸ”§ API Endpoints

### Text Processing
```json
POST /api/omni/text
Content-Type: application/json

{
  "prompt": "What's the temperature in the living room?",
  "context": "user_query",
  "max_tokens": 512
}

Response:
{
  "response": "The living room temperature is currently 22.5Â°C.",
  "confidence": 0.95,
  "processing_time": 1.2
}
```

### Image Processing
```json
POST /api/omni/image
Content-Type: multipart/form-data

{
  "image": "base64_encoded_image_data",
  "prompt": "What do you see in this image?"
}

Response:
{
  "response": "I see a living room with a sofa, coffee table, and television.",
  "objects_detected": ["sofa", "table", "television"],
  "confidence": 0.89,
  "processing_time": 2.1
}
```

### Audio Processing
```json
POST /api/omni/audio
Content-Type: audio/wav

[WAV audio data]

Response:
{
  "transcription": "What's the weather like today?",
  "response": "Today will be sunny with a high of 25Â°C.",
  "confidence": 0.92,
  "processing_time": 1.8
}
```

### Multimodal Processing
```json
POST /api/omni/multimodal
Content-Type: application/json

{
  "text": "What's in this image?",
  "image": "base64_encoded_image_data",
  "context": "visual_query"
}

Response:
{
  "response": "The image shows a kitchen with modern appliances.",
  "objects_detected": ["refrigerator", "oven", "microwave"],
  "confidence": 0.91,
  "processing_time": 2.5
}
```

## ðŸŽ¯ Features

### Natural Language Understanding
- Context-aware responses
- Multi-turn conversations
- Intent recognition
- Entity extraction

### Computer Vision
- Object detection and classification
- Scene understanding
- Activity recognition
- Text extraction from images

### Audio Processing
- Speech-to-text transcription
- Speaker identification
- Audio event detection
- Noise reduction

### Home Automation Integration
- Device control commands
- Status queries
- Scene activation
- Rule creation assistance

## âš™ï¸ Configuration

### Model Settings
```yaml
model:
  name: "Qwen3-Omni-14B-AWQ"
  quantization: "AWQ"
  context_length: 8192
  gpu_layers: 35
  temperature: 0.7
  top_p: 0.9
```

### Performance Tuning
```yaml
performance:
  batch_size: 1
  max_tokens: 512
  gpu_memory_utilization: 0.85
  tensor_parallel_size: 1
```

### Integration Settings
```yaml
integration:
  home_assistant:
    url: "http://home-assistant:8123"
    token: "your_token_here"
  frigate:
    url: "http://frigate:5000"
  mqtt:
    broker: "mqtt://mosquitto:1883"
```

## ðŸ§ª Testing

### Unit Tests
```python
def test_text_processing():
    response = omni.process_text("What time is it?")
    assert "time" in response.lower()
    assert response.confidence > 0.8

def test_image_processing():
    image_data = load_test_image()
    response = omni.process_image(image_data, "Describe this image")
    assert len(response.objects_detected) > 0
```

### Integration Tests
```python
def test_home_assistant_integration():
    response = omni.query_device_status("living_room_temperature")
    assert "temperature" in response
    assert isinstance(float(response.split()[-1]), float)
```

## ðŸ“Š Monitoring

### Health Checks
```json
GET /api/omni/health

Response:
{
  "status": "healthy",
  "model_loaded": true,
  "gpu_available": true,
  "memory_usage": "75%",
  "last_query": "2025-10-04T15:30:00Z"
}
```

### Performance Metrics
- Query processing time
- Model inference time
- Memory utilization
- GPU utilization
- Error rates

## ðŸ”’ Security

### Authentication
- API key authentication
- JWT token validation
- Rate limiting
- IP whitelisting

### Data Protection
- Encryption in transit (TLS)
- Secure model storage
- Privacy-preserving processing
- Audit logging

## ðŸš€ Deployment

### Docker Configuration
```yaml
services:
  omni:
    build:
      context: ./omni
      dockerfile: Dockerfile.qwen3-omni
    container_name: qwen3-omni
    restart: unless-stopped
    runtime: nvidia
    ports:
      - "11437:11437"
    volumes:
      - ./models:/app/model
      - ./inputs:/app/inputs
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
```

### Kubernetes Deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: omni-module
spec:
  replicas: 1
  selector:
    matchLabels:
      app: omni
  template:
    metadata:
      labels:
        app: omni
    spec:
      containers:
      - name: omni
        image: vllm/vllm-openai:latest
        resources:
          limits:
            nvidia.com/gpu: 1
```