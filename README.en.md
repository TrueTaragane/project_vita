# ðŸ§  Smart Home Architecture â€” Ryzen 7 7800X3D + RTX 4090

## ðŸŽ¯ Goal
To build a local, autonomous, high-performance smart home system with voice control, video analytics, media streaming, monitoring, and on-device AI â€” all inside a compact server.

---

## ðŸ› ï¸ Hardware Setup

### ðŸ”¹ CPU
- AMD Ryzen 7 7800X3D  
- 8 cores / 16 threads  
- High performance per watt  
- Ideal for Docker, Home Assistant, AI inference

### ðŸ”¹ GPU
- NVIDIA RTX 4090  
- 24 GB VRAM  
- Used by Frigate, Ollama, Jellyfin  
- Supports DCGM Exporter for GPU monitoring

### ðŸ”¹ Cooling
- 360mm liquid cooling (DeepCool LT720 / Arctic Liquid Freezer II / be quiet! Silent Loop 2)  
- Quiet under load  
- GPU uses stock RTX 4090 cooler

### ðŸ”¹ Case
- Fractal Design Meshify 2 / be quiet! Pure Base 500 FX  
- Excellent airflow  
- Supports 360mm radiator  
- Space for 3â€“4 HDDs + 2 SSDs

### ðŸ”¹ Power Supply
- Seasonic Focus GX 1000W / be quiet! Dark Power 13 1000W  
- 80+ Gold/Platinum certified  
- Handles RTX 4090 and sustained load

### ðŸ”¹ Storage
- NVMe SSD Gen4 2 TB â€” system, Docker, AI workloads  
- SATA SSD 4 TB â€” photo archive, Immich, Nextcloud  
- HDD 3Ã—4 TB â€” media, backups, SnapRAID parity

### ðŸ”¹ Networking
- Intel i225-V 2.5G LAN / Realtek RTL8125B  
- Jumbo Frames support  
- Connected to Keenetic / MikroTik router  
- VLAN, QoS, Wake-on-LAN

---

## âš™ï¸ Services and Roles

| Service         | Purpose                            | GPU Acceleration |
|-----------------|-------------------------------------|------------------|
| Home Assistant  | Central control hub                 | âŒ               |
| Frigate         | Video analytics (cameras, events)   | âœ…               |
| Ollama          | Local LLM (logic, dialogs)          | âœ…               |
| Whisper         | Speech-to-text (STT)                | âœ…               |
| Piper           | Text-to-speech (TTS)                | âŒ               |
| Jellyfin        | Media server, streaming, DLNA       | âœ…               |
| Samba           | NAS access via SMB                  | âŒ               |
| Portainer       | Container management                | âŒ               |
| Telegraf        | System metrics collection           | âŒ               |
| InfluxDB        | Metrics storage from Telegraf       | âŒ               |
| Prometheus      | Metrics storage from DCGM Exporter  | âŒ               |
| DCGM Exporter   | RTX 4090 monitoring                 | âœ…               |
| Grafana         | Dashboards and visualization        | âŒ               |

---

## ðŸ§  Service Interactions

- Frigate â†’ events â†’ Home Assistant  
- Home Assistant â†’ query â†’ Ollama â†’ response  
- Voice â†’ Whisper â†’ text â†’ Ollama â†’ Piper â†’ audio  
- Telegraf â†’ InfluxDB â†’ Grafana  
- DCGM Exporter â†’ Prometheus â†’ Grafana  
- Jellyfin â†’ media streaming with NVENC  
- Samba â†’ network file access

### ðŸ§  Omni Module

The stack integrates **Qwen3-Omni** â€” a multimodal agent capable of processing:
- text queries,
- images (e.g., snapshots from Frigate),
- audio (voice commands).

Omni is triggered by events and optimized for RTX 4090 (24 GB VRAM).

---

## ðŸ“¦ Storage Layout

| Disk           | Role                          | Mount Point       |
|----------------|-------------------------------|-------------------|
| SSD 2 TB       | System, Docker, AI workloads  | /mnt/system       |
| HDD #1 4 TB    | Media, NAS files              | /mnt/media        |
| HDD #2 4 TB    | Backups (BorgBackup)          | /mnt/backup       |
| HDD #3 4 TB    | SnapRAID parity               | /mnt/parity       |

---

## ðŸ“Š Monitoring

- Telegraf: CPU, RAM, disks, network, containers  
- DCGM Exporter: temperature, load, VRAM of RTX 4090  
- Grafana: dashboards and alerts  
- Prometheus: GPU metrics

---

## ðŸ—£ï¸ Voice Assistant

- Whisper â†’ STT  
- Ollama â†’ logic  
- Piper â†’ TTS  
- Home Assistant â†’ routing  
- Supports dialogs, commands, scenarios

---

## ðŸ“ Docker Compose

All services are defined in `docker-compose.yaml`, including GPU passthrough, Docker socket, Telegraf and Prometheus configs.

---

## ðŸ“ˆ Extensions

- Immich â€” AI-powered photo archive  
- Nextcloud â€” personal cloud and documents  
- Node-RED â€” visual automation logic  
- Telegram Alerts â€” notifications  
- Watchtower â€” automatic container updates

---

## ðŸ§  Philosophy

Maximum autonomy, local AI, visual control, data protection, flexibility, and scalability.

---

## ðŸ“š Additional Materials

- [ðŸ“ Limitations & Caveats](docs/limitations.md)
- [ðŸ”® Roadmap & Future Plans](docs/roadmap.md)
- [ðŸ§  Omni Module Documentation](docs/omni-module.en.md)
- [ðŸŽ™ï¸ Voice Assistant](docs/voice-assistant.en.md)
- [ðŸ› ï¸ Makefile Guide](docs/makefile-guide.en.md)

- ### ðŸ§  Comparison: Qwen3â€‘Omni vs Qwen2.5â€‘Omni
> âš ï¸ Qwen3â€‘Omniâ€‘14B is not officially hosted on Hugging Face.  
> It is available via vLLM-compatible AWQ format from community sources.



| Feature                     | Qwen3â€‘Omniâ€‘14B                                                             | Qwen2.5â€‘Omni (72B)                                                                  |
|----------------------------|----------------------------------------------------------------------------|-------------------------------------------------------------------------------------|
| Model size                 | 14B                                                                        | 72B                                                                                 |
| Multimodal capabilities     | âœ… Text, image, audio                                                      | âœ… Text, image, audio                                                               |
| Audio support               | âœ…                                                                          | âœ…                                                                                  |
| Image support               | âœ…                                                                          | âœ…                                                                                  |
| Compatibility               | [vLLM](https://github.com/vllm-project/vllm), [AWQ](https://github.com/mit-han-lab/awq), Docker | [HuggingFace Transformers](https://github.com/QwenLM/Qwen2.5-Omni)                 |
| Runs on RTX 4090            | âœ… Fully compatible                                                        | âš ï¸ Requires 2Ã— RTX 4090 or A100                                                     |
| VRAM usage                  | ~22â€“24 GB                                                                  | ~48â€“80 GB                                                                           |
| Response speed              | âš¡ Fast (AWQ + vLLM)                                                        | ðŸ§  Slower, higher resource demand                                                   |
| Suitable for local AI       | âœ… Yes                                                                     | âš ï¸ Only with datacenter-grade hardware                                              |
| Stability                   | Proven in production stack                                                 | New, experimental                                                                  |

> ðŸ’¡ **Why Qwen3â€‘Omni was chosen**:  
> Itâ€™s ideal for a single RTX 4090 setup, fully compatible with vLLM and Docker, and already integrated into the architecture.  
> Qwen2.5â€‘Omni is a next-generation model for scaling, but requires significantly more resources.


![Yuri's AI-Powered Docker Server Architecture](https://github.com/TrueTaragane/smart-home-architecture/blob/main/githubusercontent.png?raw=true)
