version: "3.9"
services:
  llama-cpp:
    image: ghcr.io/ggerganov/llama.cpp:latest
    container_name: llama-cpp
    restart: unless-stopped
    ports:
      - "${PORT_LLAMA_CPP}:8080"
    volumes:
      - ${MODEL_LLAMA_CPP}:/models
    command: --model /models --port 8080 --host 0.0.0.0
    # This service is only started when GPU is not available or not needed
    
  # GPU-enabled version (optional)
  llama-cpp-gpu:
    image: ghcr.io/ggerganov/llama.cpp:latest
    container_name: llama-cpp-gpu
    restart: unless-stopped
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=${GPU_DEVICE:-0}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "${PORT_LLAMA_CPP_GPU}:8080"
    volumes:
      - ${MODEL_LLAMA_CPP}:/models
    command: --model /models --port 8080 --host 0.0.0.0 --n-gpu-layers 35